# FedICT

This repository is the official Pytorch implementation DEMO of **FedICT**:

[**FedICT: Federated Multi-task Distillation for Multi-access Edge Computing**.](FedICT: Federated Multi-task Distillation for Multi-access Edge Computing) *IEEE Transactions on Parallel and Distributed Systems (TPDS)*. 2024

-------
## Run this DEMO
```python main_fedict.py```

-------

## Cite this work

```bibtex
@ARTICLE{wu2024fedict,
  author={Wu, Zhiyuan and Sun, Sheng and Wang, Yuwei and Liu, Min and Pan, Quyang and Jiang, Xuefeng and Gao, Bo},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={FedICT: Federated Multi-Task Distillation for Multi-Access Edge Computing}, 
  year={2024},
  volume={35},
  number={6},
  pages={1107-1121},
  keywords={Computational modeling;Data models;Training;Servers;Multitasking;Adaptation models;Optimization;Distributed optimization;federated learning;knowledge distillation;multi-access edge computing;multi-task learning},
  doi={10.1109/TPDS.2023.3289444}}
```

## Related Works

[FedCache: A Knowledge Cache-driven Federated Learning Architecture for Personalized Edge Intelligence.](https://ieeexplore.ieee.org/document/10420495) *IEEE Transactions on Mobile Computing (TMC)*. 2024

[Agglomerative Federated Learning: Empowering Larger Model Training via End-Edge-Cloud Collaboration.](https://arxiv.org/abs/2312.11489) *IEEE International Conference on Computer Communications (INFOCOM).* 2024

[Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation.](https://dl.acm.org/doi/10.1145/3639369) *ACM Transactions on Intelligent Systems and Technology (TIST)*. 2024.

[FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset Distillation.](https://arxiv.org/abs/2405.13378) *arXiv preprint arXiv:2405.13378*. 2024.

[Privacy-Enhanced Training-as-a-Service for On-Device Intelligence: Concept, Architectural Scheme, and Open Problems.](https://arxiv.org/abs/2404.10255) *arXiv preprint arXiv:2404.10255*. 2024.

[Federated Class-Incremental Learning with New-Class Augmented Self-Distillation.](https://arxiv.org/abs/2401.00622) *arXiv preprint arXiv:2401.00622.* 2024.

[Knowledge Distillation in Federated Edge Learning: A Survey.](https://arxiv.org/abs/2301.05849) *arXiv preprint arXiv:2301.05849.* 2023.